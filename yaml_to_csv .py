{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bfb0aeb-f257-444d-94e6-364a8a8fe93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from 'C:\\Users\\Hp\\OneDrive\\ドキュメント\\file.yaml'...\n",
      "Processing month: 2023-10\n",
      "Processing month: 2023-11\n",
      "Processing month: 2023-12\n",
      "Processing month: 2024-01\n",
      "Processing month: 2024-02\n",
      "Processing month: 2024-03\n",
      "Processing month: 2024-04\n",
      "Processing month: 2024-05\n",
      "Processing month: 2024-06\n",
      "Processing month: 2024-07\n",
      "Processing month: 2024-08\n",
      "Processing month: 2024-09\n",
      "Processing month: 2024-10\n",
      "Processing month: 2024-11\n",
      "\n",
      "Master DataFrame created with 14200 rows.\n",
      "Sample of Master DataFrame:\n",
      "     Symbol    Close                Date     High      Low     Open   Volume\n",
      "0  ADANIENT  2387.25 2023-10-03 05:30:00  2424.90  2372.00  2418.00  2019899\n",
      "1  ADANIENT  2464.95 2023-10-04 05:30:00  2502.75  2392.25  2402.20  2857377\n",
      "2  ADANIENT  2466.35 2023-10-05 05:30:00  2486.50  2446.40  2477.95  1132455\n",
      "3  ADANIENT  2478.10 2023-10-06 05:30:00  2514.95  2466.05  2466.35  1510035\n",
      "4  ADANIENT  2442.60 2023-10-09 05:30:00  2459.70  2411.30  2440.00  1408224\n",
      "\n",
      "Saving individual CSV files to 'stock_data_csvs' directory...\n",
      "Saved ADANIENT.csv with 284 rows.\n",
      "Saved ADANIPORTS.csv with 284 rows.\n",
      "Saved APOLLOHOSP.csv with 284 rows.\n",
      "Saved ASIANPAINT.csv with 284 rows.\n",
      "Saved AXISBANK.csv with 284 rows.\n",
      "Saved BAJAJ-AUTO.csv with 284 rows.\n",
      "Saved BAJAJFINSV.csv with 284 rows.\n",
      "Saved BAJFINANCE.csv with 284 rows.\n",
      "Saved BEL.csv with 284 rows.\n",
      "Saved BHARTIARTL.csv with 284 rows.\n",
      "Saved BPCL.csv with 284 rows.\n",
      "Saved BRITANNIA.csv with 284 rows.\n",
      "Saved CIPLA.csv with 284 rows.\n",
      "Saved COALINDIA.csv with 284 rows.\n",
      "Saved DRREDDY.csv with 284 rows.\n",
      "Saved EICHERMOT.csv with 284 rows.\n",
      "Saved GRASIM.csv with 284 rows.\n",
      "Saved HCLTECH.csv with 284 rows.\n",
      "Saved HDFCBANK.csv with 284 rows.\n",
      "Saved HDFCLIFE.csv with 284 rows.\n",
      "Saved HEROMOTOCO.csv with 284 rows.\n",
      "Saved HINDALCO.csv with 284 rows.\n",
      "Saved HINDUNILVR.csv with 284 rows.\n",
      "Saved ICICIBANK.csv with 284 rows.\n",
      "Saved INDUSINDBK.csv with 284 rows.\n",
      "Saved INFY.csv with 284 rows.\n",
      "Saved ITC.csv with 284 rows.\n",
      "Saved JSWSTEEL.csv with 284 rows.\n",
      "Saved KOTAKBANK.csv with 284 rows.\n",
      "Saved LT.csv with 284 rows.\n",
      "Saved M&M.csv with 284 rows.\n",
      "Saved MARUTI.csv with 284 rows.\n",
      "Saved NESTLEIND.csv with 284 rows.\n",
      "Saved NTPC.csv with 284 rows.\n",
      "Saved ONGC.csv with 284 rows.\n",
      "Saved POWERGRID.csv with 284 rows.\n",
      "Saved RELIANCE.csv with 284 rows.\n",
      "Saved SBILIFE.csv with 284 rows.\n",
      "Saved SBIN.csv with 284 rows.\n",
      "Saved SHRIRAMFIN.csv with 284 rows.\n",
      "Saved SUNPHARMA.csv with 284 rows.\n",
      "Saved TATACONSUM.csv with 284 rows.\n",
      "Saved TATAMOTORS.csv with 284 rows.\n",
      "Saved TATASTEEL.csv with 284 rows.\n",
      "Saved TCS.csv with 284 rows.\n",
      "Saved TECHM.csv with 284 rows.\n",
      "Saved TITAN.csv with 284 rows.\n",
      "Saved TRENT.csv with 284 rows.\n",
      "Saved ULTRACEMCO.csv with 284 rows.\n",
      "Saved WIPRO.csv with 284 rows.\n",
      "\n",
      "Data extraction and transformation complete!\n",
      "You can find the generated CSV files in the 'stock_data_csvs' directory.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# This is the base directory where you will extract your data.rar content.\n",
    "# For example, if you extract data.rar to 'my_stock_data',\n",
    "# then inside 'my_stock_data' you'd have '2023-10', '2023-11', etc.\n",
    "# UPDATED PATH BASED ON YOUR INPUT:\n",
    "BASE_DATA_DIR = r'C:\\Users\\Hp\\OneDrive\\ドキュメント\\file.yaml' # <--- UPDATED PATH\n",
    "\n",
    "# --- Simulate Data Structure for Testing (if you don't have actual files yet) ---\n",
    "# This dictionary mimics the structure after you extract data.rar\n",
    "# and assumes YAML files like '2023-10-03.yaml' exist within month folders.\n",
    "# You will replace this with actual file system reading.\n",
    "dummy_data_structure = {\n",
    "    '2023-10': {\n",
    "        '2023-10-03.yaml': \"\"\"\n",
    "- Ticker: SBIN\n",
    "  close: 602.95\n",
    "  date: '2023-10-03 05:30:00'\n",
    "  high: 604.9\n",
    "  low: 589.6\n",
    "  month: 2023-10\n",
    "  open: 596.6\n",
    "  volume: 15322196\n",
    "- Ticker: BAJFINANCE\n",
    "  close: 7967.6\n",
    "  date: '2023-10-03 05:30:00'\n",
    "  high: 7975.5\n",
    "  low: 7755.0\n",
    "  month: 2023-10\n",
    "  open: 7780.8\n",
    "  volume: 944555\n",
    "- Ticker: RELIANCE\n",
    "  close: 1159.08\n",
    "  date: '2023-10-03 05:30:00'\n",
    "  high: 1167.8\n",
    "  low: 1158.0\n",
    "  month: 2023-10\n",
    "  open: 1164.97\n",
    "  volume: 8859056\n",
    "- Ticker: TCS\n",
    "  close: 3513.85\n",
    "  date: '2023-10-03 05:30:00'\n",
    "  high: 3534.2\n",
    "  low: 3480.1\n",
    "  month: 2023-10\n",
    "  open: 3534.2\n",
    "  volume: 1948148\n",
    "\"\"\"\n",
    "    },\n",
    "    '2023-11': {\n",
    "        '2023-11-01.yaml': \"\"\"\n",
    "- Ticker: SBIN\n",
    "  close: 610.0\n",
    "  date: '2023-11-01 05:30:00'\n",
    "  high: 615.0\n",
    "  low: 600.0\n",
    "  month: 2023-11\n",
    "  open: 605.0\n",
    "  volume: 16000000\n",
    "- Ticker: RELIANCE\n",
    "  close: 1170.0\n",
    "  date: '2023-11-01 05:30:00'\n",
    "  high: 1180.0\n",
    "  low: 1160.0\n",
    "  month: 2023-11\n",
    "  open: 1165.0\n",
    "  volume: 9000000\n",
    "\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Function to read data from actual files ---\n",
    "def read_data_from_files(base_dir):\n",
    "    \"\"\"\n",
    "    Reads YAML data from the specified directory structure (month_folder/date_file.yaml).\n",
    "    Returns a list of dictionaries, where each dict is a stock entry for a specific day.\n",
    "    \"\"\"\n",
    "    all_stock_entries = []\n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"Error: Base data directory '{base_dir}' not found. Please ensure the path is correct.\")\n",
    "        print(\"Using dummy data for demonstration.\")\n",
    "        # Fallback to dummy data if directory not found\n",
    "        for month_folder, date_files in dummy_data_structure.items():\n",
    "            for filename, content in date_files.items():\n",
    "                try:\n",
    "                    # Load each YAML content string as a list of dictionaries\n",
    "                    daily_entries = yaml.safe_load(content)\n",
    "                    all_stock_entries.extend(daily_entries)\n",
    "                except yaml.YAMLError as e:\n",
    "                    print(f\"Error parsing dummy YAML content for {month_folder}/{filename}: {e}\")\n",
    "        return all_stock_entries\n",
    "\n",
    "    print(f\"Reading data from '{base_dir}'...\")\n",
    "    # List directories and sort them to ensure consistent processing order (e.g., '2023-10' before '2023-11')\n",
    "    for month_folder in sorted(os.listdir(base_dir)):\n",
    "        month_path = os.path.join(base_dir, month_folder)\n",
    "        if os.path.isdir(month_path): # Ensure it's a directory\n",
    "            print(f\"Processing month: {month_folder}\")\n",
    "            # List files within the month directory and sort them\n",
    "            for date_file in sorted(os.listdir(month_path)):\n",
    "                if date_file.endswith('.yaml') or date_file.endswith('.yml'):\n",
    "                    file_path = os.path.join(month_path, date_file)\n",
    "                    try:\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            # Each YAML file is a list of stock entries for that date\n",
    "                            daily_entries = yaml.safe_load(f)\n",
    "                            if daily_entries: # Ensure the loaded data is not empty\n",
    "                                all_stock_entries.extend(daily_entries)\n",
    "                    except FileNotFoundError:\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "                    except yaml.YAMLError as e:\n",
    "                        print(f\"Error parsing YAML file {file_path}: {e}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"An unexpected error occurred with {file_path}: {e}\")\n",
    "    return all_stock_entries\n",
    "\n",
    "# --- Main Execution ---\n",
    "output_dir = 'stock_data_csvs'\n",
    "# Create the output directory if it doesn't already exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read all data from files (or use dummy data if BASE_DATA_DIR not found)\n",
    "all_raw_data = read_data_from_files(BASE_DATA_DIR)\n",
    "\n",
    "if not all_raw_data:\n",
    "    print(\"No data loaded. Exiting data extraction process.\")\n",
    "    # If no data is loaded, we can't proceed with DataFrame creation or saving.\n",
    "    exit()\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "master_df = pd.DataFrame(all_raw_data)\n",
    "\n",
    "# Rename columns to be consistent with common conventions (optional but good practice)\n",
    "master_df.rename(columns={\n",
    "    'Ticker': 'Symbol',\n",
    "    'close': 'Close',\n",
    "    'high': 'High',\n",
    "    'low': 'Low',\n",
    "    'open': 'Open',\n",
    "    'volume': 'Volume'\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert 'date' column to datetime objects and extract just the date part\n",
    "# Using .dt.date converts it to a Python date object, which is fine for storage,\n",
    "# but for further calculations with pandas, it's often better to keep it as datetime.\n",
    "# Let's keep it as datetime for now, and only extract date if strictly needed later.\n",
    "master_df['date'] = pd.to_datetime(master_df['date'])\n",
    "master_df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "\n",
    "# Drop the 'month' column as it's redundant now that we have 'Date'\n",
    "if 'month' in master_df.columns:\n",
    "    master_df.drop(columns=['month'], inplace=True)\n",
    "\n",
    "# Sort by Symbol and Date for consistency\n",
    "master_df = master_df.sort_values(by=['Symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nMaster DataFrame created with {len(master_df)} rows.\")\n",
    "print(\"Sample of Master DataFrame:\")\n",
    "print(master_df.head())\n",
    "\n",
    "# --- Save individual CSV files for each symbol ---\n",
    "print(f\"\\nSaving individual CSV files to '{output_dir}' directory...\")\n",
    "unique_symbols = master_df['Symbol'].unique()\n",
    "\n",
    "for symbol in unique_symbols:\n",
    "    # Select data for the current symbol\n",
    "    symbol_df = master_df[master_df['Symbol'] == symbol].copy()\n",
    "    csv_filename = os.path.join(output_dir, f'{symbol}.csv')\n",
    "    # Save the DataFrame to a CSV file without the index\n",
    "    symbol_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Saved {symbol}.csv with {len(symbol_df)} rows.\")\n",
    "\n",
    "print(\"\\nData extraction and transformation complete!\")\n",
    "print(f\"You can find the generated CSV files in the '{output_dir}' directory.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a240cbd7-ec2a-40de-ba9a-d86562f6c057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
